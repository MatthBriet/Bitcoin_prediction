{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2d923511-00c9-47ff-841e-64935e9c41e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Enseignement d'intégration - Bitcoin Trading Bot - Groupe 7\n",
    "## Bitcoin Forecasting using Deep Learning\n",
    "### COLLEVILLE Tanguy, BOULAY Clément"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CentraleSupelec Logo](https://www.centralesupelec.fr/sites/all/themes/cs_theme/medias/common/images/intro/logo_nouveau.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "__author__ = \"Colleville Tanguy\", \"Clément Boulay\"\n",
    "__version__ = \"1.0.0\"\n",
    "__maintainer__ = \"Colleville Tanguy\"\n",
    "__email__ = [\"tanguy.colleville@student-cs.fr\", \"clement.boulay@student-cs.fr\"]\n",
    "__status__ = \"Dev\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 - Importation des librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dcca5a38-d51c-4bf6-82b4-a2264949f16a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import time as time\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.layers import Dense,Bidirectional,Dropout,LSTM,BatchNormalization,Flatten,Conv1D\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.initializers import HeNormal,GlorotNormal\n",
    "from scipy.stats import reciprocal\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.model_selection import RandomizedSearchCV,GridSearchCV\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import  StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "CURDIR = os.path.dirname(os.getcwd())\n",
    "DATADIR = os.path.join(CURDIR,  \"data\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#paramètres de représentation\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "plt.rcParams['font.size'] = 10\n",
    "plt.rcParams['axes.labelsize'] = 10\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['xtick.labelsize'] = 8\n",
    "plt.rcParams['ytick.labelsize'] = 8\n",
    "plt.rcParams['legend.fontsize'] = 10\n",
    "plt.rcParams['figure.titlesize'] = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "norme : pep8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d99ae8fb-f679-4b74-9b2b-54facd666a81",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 1.1 - Importation des données\n",
    "Les données proviennent du site https://blockchain.info/."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b1dd7725-d08b-477d-a5d0-0fa210b411bc",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_blockchain = pd.read_csv(os.path.join(DATADIR, \"df_blockchain.csv\"), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 - Découverte des données\n",
    "Les questions auxquelles on cherche à répondre ici sont :\n",
    "- Quelles sont les variables explicatives, que représentent-elles ? Qui est la variable à expliquer, que représente-t-elle ?\n",
    "- De quelle forme est le jeu de données (types des variables, shape, complet, incomplet) ?\n",
    "- Peut-on extraire une corrélation entre certaines variables ? Lesquelles choisir pour la modélisation ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e3cf49b8-1239-4d94-a951-ba912f09861c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_blockchain.head()## affichage 5 premières lignes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blockchain.columns.sort_values().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blockchain.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_blockchain.info(memory_usage='deep'))## donne le vrai usage de mémoire du df \n",
    "print(df_blockchain.memory_usage(deep=True))## stockage par série"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_blockchain.isna().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_blockchain.shape)\n",
    "df_blockchain.dropna(axis=0,inplace=True)## drop les lignes où y'a des nan \n",
    "print(df_blockchain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blockchain.dtypes.value_counts().plot.pie(title=\"features' type\")# petit camenbert des différents type du dataframe\n",
    "plt.xlabel(\" \")\n",
    "plt.ylabel(\" \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Signification des variables (d'après https://blockchain.info/) :\n",
    "![notebook\\features.png](attachment:notebook\\features.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_blockchain.select_dtypes('float'):\n",
    "    plt.title(f\"Distribution of {col}\")\n",
    "    sns.distplot(df_blockchain[col])\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que certaines distributions sont piquées (trade-volume, transaction-fees, estimated-transaction-volume, output_volume) alors que d'autres sont étalées (avg-block-size, n-transactions-per-block). Pour la distribution du market-price, on observe un fort pic autour de 0, qui correspond aux premières années du Bitcoin (quand il était à moins de 100$). On retrouve ce pic caractéristique autour de 0 dans plusieurs autres distributions (trade-volume, miners-revenue, difficulty, hash-rate), avec toujours la même signification temporelle. Enfin, pour certaines distributions, on peut clairement voir une évolution temporelle (\"de gauche à droite\") : c'est le cas pour total-bitcoins (valeurs présentes à droite du graphe) et difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_blockchain.select_dtypes('object'):\n",
    "    print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Price=df_blockchain[\"market-price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Price.values)\n",
    "plt.title(\"Price of BTC evolution\")\n",
    "plt.ylabel(\"BTC price, $ USD\" )\n",
    "plt.xlabel(\"days\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 - Exploration des liens entre les données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df_blockchain[\"total-bitcoins\"]/500,label=\"number of BTC mined \")\n",
    "plt.plot(df_blockchain[\"market-price\"],label=\"price\")\n",
    "plt.title(\"Number of BTC mined and price evolution over time\")\n",
    "plt.xlabel(\"days\")\n",
    "plt.ylabel(\"USD $ and number of BTC/\")\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(Price[1750:-30],label=\"what we have for training\")\n",
    "plt.plot(Price[-30:],label=\"What we want to predict\")\n",
    "plt.title(\"Price of BTC evolution\")\n",
    "plt.ylabel(\"BTC price, $ USD\" )\n",
    "plt.xlabel(\"days\")\n",
    "plt.legend()\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut penser que la variation brusque du cours du Bitcoin sur la période de test va poser des problèmes dans la mesure où cette chute de prix n'a pas eu lieu par le passé donc est imprévisible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexed=df_blockchain.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexed.index = pd.to_datetime(df_blockchain.set_index(\"Date\").index)\n",
    "min_year=df_indexed.index.year.min()\n",
    "max_year=df_indexed.index.year.max()\n",
    "print(\"min year\", df_indexed.index.year.min())\n",
    "print(\"max year\",df_indexed.index.year.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_indexed.index = pd.to_datetime(df_blockchain.set_index(\"Date\").index)\n",
    "df_blockchain[\"Year\"]=df_indexed.index.year\n",
    "df_blockchain[\"Month\"]=df_indexed.index.month\n",
    "df_blockchain[\"Day\"]=df_indexed.index.day\n",
    "df_blockchain[\"week_day\"]=df_indexed.index.weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap=plt.cm.gist_ncar\n",
    "colost=[colormap(i) for i in np.linspace(0,0.95,1+max_year-min_year)]## make a nice colorbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.title(\"Market price over year\")\n",
    "for i in range(min_year,max_year+1,1):\n",
    "    plt.plot(df_blockchain[df_blockchain[\"Year\"]==i][\"market-price\"].values,color=colost[i-min_year],label=i)\n",
    "plt.legend()\n",
    "plt.xlabel(\"days\")\n",
    "plt.ylabel(\"USD $\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On ne voit pas de saisonnalité dans les prix même si on peut remarquer un engoument certain pour le btc ces dernières années. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blockchain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in df_blockchain.columns[2:-4]:\n",
    "    plt.figure()\n",
    "    for i in range(min_year,max_year+1,1):\n",
    "        plt.title(f\"{j} over year\")\n",
    "        plt.plot(df_blockchain[df_blockchain[\"Year\"]==i][j].values,color=colost[i-min_year],label=i)\n",
    "        plt.xlabel(\"days\")\n",
    "        plt.ylabel(j)\n",
    "        plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette série de graphes montre l'évolution de chacune des variables au travers des années. Certaines représentations sont particulièrement intéressantes. Par exemple, on observe que le hash-rate augmente de façon cosntante depuis 2016. C'est également le cas pour la difficulté et la blocks-size (qui augmente linéairement, ce qui est logique puisque la taille de la blockchain ne fait que croître). D'autre part, certains graphes indiquent que l'année 2021 est particulière pour le marché du Bitcoin : le cost-per-transaction est par exemple plus élevé cette année qu'auparavant (facteur 2.5 au plus haut !). De la même façon, ces graphes permettent d'identifier des moments marquants de l'histoire du Bitcoin. À ce titre, on identifie un fort volume de transaction en février-mars 2011 (en Bitcoin, cela n'apparaît pas en USD car à cette époque le BTC ne valait rien, c'est le moment de la parité avec l'USD), ainsi que depuis 2020 dans l'échelle USD (qui ne traduit pas forcément plus de transactions mais traduit un prix qui explose). Enfin, on observe une explosion de la capitalisation \"market-cap\" depuis deux ans, qui montre l'attrait pour les cryptomonnaies. D'une façon générale, on ne remarque pas de saisonnalité dans les données."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=\"market-price\"\n",
    "for i in df_blockchain.select_dtypes('float'): \n",
    "    print(i)\n",
    "    plt.figure()\n",
    "    plt.title(f\"{i} and {target}\")\n",
    "    plt.scatter(df_blockchain[i],df_blockchain[target])\n",
    "    plt.xlabel(i)\n",
    "    plt.ylabel(target)\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La série de graphes suivante montre les variations de chacune des 22 variables explicatives en fonction de la variable à expliquer \"market-price\". De façon notable, on remarque une corrélation entre cette dernière et les variables suivantes : miners-revenue, transaction-fees-usd, cost-per-transaction, estimated-transaction-volume-usd, market-cap (de façon notable).\n",
    "De plus, pour les variables explicatives dont le comportement est approximativement linéaire du temps (difficulty, hash-rate au début, blocks-size, n-transactions-total), on retrouve la forme du graphe des prix représenté au-dessus (cela revient indirectement à représenter le prix en fonction du temps)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 - Analyse des corrélations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,7))\n",
    "plt.title(\"Correlation heatmap\")\n",
    "sns.heatmap(df_blockchain.corr(),cmap=\"seismic\",annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix=df_blockchain.corr()\n",
    "sol = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool)).stack().sort_values(ascending=False))\n",
    "p=0\n",
    "coupled=[]\n",
    "for index, value in sol.items():\n",
    "    if value>0.9:\n",
    "        p+=1\n",
    "        coupled.append(index)\n",
    "print(p,\"features have more than 90% of correlation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coupled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plusieurs approches sont possibles : nous les mettrons en oeuvre par la suite lors de notre étape de preprocessing ici des tris sont fait simplement pour voir si la suppresion de variables \"parasites\" révèlent des tendances ou des relations \"cachées\"\n",
    "* la première serait de supprimer les variables corrélées entre elles dans la mesure où l'une explique l'autre et d'enlever les variables fortement corrélée avec la target surtout celle qui dérive de market-price. \n",
    "* la deuxième est d'utiliser un random forest pour mesurer l'importance des features ou encore utiliser une regression LASSO \n",
    "* la troisième est d'étudier les valeurs propres de la matrice de corrélation pour trier. En effet, de faibles valeurs propres indiques que des features sont redondants car la famille devient liée on peut ainsi évincer certains features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blockchain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_to_be_dropped=[\"market-cap\",\"difficulty\",\"n-unique-addresses\",\"n-transactions-excluding-popular\",\"estimated-transaction-volume-usd\"]\n",
    "df_blockchain.drop(features_to_be_dropped,axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16,7))\n",
    "plt.title(\"Correlation heatmap with less features\")\n",
    "sns.heatmap(df_blockchain.corr(),cmap=\"seismic\",annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "for i in range(0,7,1):\n",
    "    plt.title(f\"market price over week day \")\n",
    "    plt.plot(df_blockchain[df_blockchain[\"week_day\"]==i][\"market-price\"].values,color=colost[i],label=i,alpha=0.7)  \n",
    "plt.legend()\n",
    "plt.ylabel(\"$ USD\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visiblement il n'y pas tant de traders du \"dimanche\". En effet, au vu de l'engouement des particuliers pour cette crypto nous aurions pu nous attendre à voir des tendances le week end par exemple mais ce n'est pas le cas. NB :  0 Lundi, 1 Mardi etc.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colormap=plt.cm.gist_ncar\n",
    "colostm=[colormap(i) for i in np.linspace(0,0.95,1+12)]## len du nb de trucs à plot\n",
    "\n",
    "plt.figure()\n",
    "for i in range(0,13,1):\n",
    "    plt.title(f\"market price over month \")\n",
    "    plt.plot(df_blockchain[df_blockchain[\"Month\"]==i][\"market-price\"].values,label=i,color=colostm[i])\n",
    "plt.legend()\n",
    "plt.ylabel(\"BTC price $ USD\")\n",
    "plt.xlabel(\"days\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les mois non plus ne sont dégages pas de tendances sur le prix. Ceci vient confirmer la correlation matrix illustrée par la heatmap dans laquelle on voit que les correlations sont faibles pour les jours de la semaine et les mois. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 - Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "08350022-3527-4c51-9261-97f26fde3619",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Le \"preprocessing\" (en français, préparation préliminaire) est une méthode utilisée en Machine Learning dans le but de mettre en forme les données afin qu'elles correspondent au format d'entrée des modèles (par ex. réseaux de neurones). Cette phase de préparation est essentielle et permet d'optimiser l'efficacité des algorithmes employés ensuite. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le Rasoir d'Ockham est un principe de raisonnement philosophique entrant dans les concepts de rationalisme et de nominalisme. Il peut se comprendre par la locution suivante \"les multiples ne doivent pas être utilisés sans nécessité\". C'est un principe phare du machine learning dans la mesure où nous cherchons toujours les modèles les plus rapides et les plus parcimonieux pour un même résultat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imputation(df):\n",
    "    \"\"\"\n",
    "    Entries : \n",
    "        df (pandas DataFrame) : tableau contenant les données brutes (présence de NaN)\n",
    "    ====================================================================================================\n",
    "    Aim : \n",
    "        Retirer les valeurs NaN des données du tableau\n",
    "    ====================================================================================================\n",
    "    Outputs: \n",
    "        df (pandas DataFrame) : tableau de données nettoyé des valeurs NaN\n",
    "    \"\"\"\n",
    "    df.dropna(axis=0,inplace=True)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encodage(df):\n",
    "    # rien ici car le jeu de données est déjà au bon format (float64)\n",
    "    return df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_relevant_features(df,features):\n",
    "\n",
    "    \"\"\"\n",
    "    Entries :\n",
    "    df (pandas DataFrame) : tableau contenant toutes les variables explicatives\n",
    "    features (list) : variables explicatives à garder (celles peu corrélées entre elles)\n",
    "    ====================================================================================================\n",
    "    Aim :\n",
    "    Identifier toutes les variables explicatives redondantes (corrélées entre elles) du tableau, qui n'apportent pas d'information\n",
    "    ====================================================================================================\n",
    "    Outputs:\n",
    "    my_features (list) : variables explicatives à garder pour l'étude\n",
    "    \"\"\"\n",
    "\n",
    "    myfeatures=features\n",
    "    correlation=df.corr()\n",
    "\n",
    "    colonnes=df.columns.drop(\"Date\").drop(\"market-price\")\n",
    "\n",
    "    for ligne in colonnes:\n",
    "\n",
    "        for col in colonnes:\n",
    "\n",
    "            if 0.9<correlation[ligne][col]<1:\n",
    "\n",
    "                if col in myfeatures:\n",
    "\n",
    "                    myfeatures=myfeatures.drop(col)\n",
    "\n",
    "            elif correlation[ligne][col]==1:\n",
    "                break\n",
    "    return myfeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_engineering(df):  \n",
    "    \"\"\"\n",
    "    Entries :\n",
    "    df (pandas.DataFrame) : tableau de données complet\n",
    "    ====================================================================================================\n",
    "    Aim :\n",
    "    Créer de nouvelles variables explicatives et retirer celles trop corrélées\n",
    "    ====================================================================================================\n",
    "    Outputs :\n",
    "    df (pandas.Dataframe) : tableau de données avec de nouvelles variables apportant de l'information\n",
    "    \"\"\"\n",
    "    ########################################################## DATE ##################################################################\n",
    "    features_to_be_dropped=[\"market-cap\",\"difficulty\",\"n-unique-addresses\",\"n-transactions-excluding-popular\",\"estimated-transaction-volume-usd\",\"n-transactions-total\"]# variables à éliminer selon une analyse visuelle de la matrice de corrélation\n",
    "    relevant_feature=keep_relevant_features(df,df.columns)\n",
    "    for col in df.columns:\n",
    "        if col not in relevant_feature and col!=\"market-price\" and col!=\"Date\": # on élimine les colonnes identifiées\n",
    "            df=df.drop(columns=[col])\n",
    "    for col in features_to_be_dropped : # on vérifie a-posteriori que le tri s'est bien déroulé\n",
    "        if col in df.columns : \n",
    "            df=df.drop(columns=[col])\n",
    "\n",
    "    # création de nouvelles variables à partir de la date\n",
    "    df_indexed=df.set_index(\"Date\")\n",
    "    df_indexed.index = pd.to_datetime(df.set_index(\"Date\").index)\n",
    "    df[\"Year\"]=df_indexed.index.year\n",
    "    df[\"Month\"]=df_indexed.index.month\n",
    "    df[\"Day\"]=df_indexed.index.day\n",
    "    df[\"week_day\"]=df_indexed.index.weekday\n",
    "    df.index = pd.to_datetime(df.set_index(\"Date\").index)# on retire la date car l'information est désormais stockée\n",
    "    df.drop(\"Date\",axis=1,inplace=True)\n",
    "    ## l'étude de l'importance des features grâce à random forest [dans l'autre notebook] a confirmé la selection des features déjà réalisée\n",
    "    # avec plus de temps : \n",
    "    ## à partir de regression on aurait pu créer d'autres features : combinaison de features \n",
    "    ## on aurait pu également faire des moyennes glissantes sur des plages de temps variées (fenêtrage)\n",
    "    return(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaler(df):\n",
    "    \"\"\"\n",
    "    Entries : \n",
    "        df (pandas DataFrame) : tableau contenant les données brutes (non-standardisées)\n",
    "    ====================================================================================================\n",
    "    Aim : \n",
    "        Retourner un tableau de données standardisé : pour chaque variable explicative,\n",
    "        moyenne nulle et écart-type égal à 1.\n",
    "    \n",
    "    Outputs: \n",
    "        df (pandas DataFrame) : tableau de données standardisé\n",
    "    \"\"\"\n",
    "    scale = StandardScaler()\n",
    "    market_price = df.pop(\"market-price\")\n",
    "    market_price = market_price.reset_index(drop=True)\n",
    "    columns = df.columns\n",
    "    df = scale.fit_transform(df)\n",
    "    df = pd.DataFrame(df, columns = columns)\n",
    "    df.insert(0, \"market-price\", market_price)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    \"\"\"\n",
    "    Entries : \n",
    "        df (pandas DataFrame) : tableau contenant les données brutes\n",
    "    ====================================================================================================\n",
    "    Aim : \n",
    "        Retourner un tableau de données prêt à l'emploi (standardisé, sans NaN, avec des variables explicatives\n",
    "        sans grande corrélation)\n",
    "    ====================================================================================================\n",
    "    Outputs: \n",
    "        X -->pandas Dataframe df sans market price\n",
    "        Y -->pandas serie market price \n",
    "        df (pandas DataFrame) : tableau de données prêt à l'emploi\n",
    "    \"\"\"\n",
    "    df = imputation(df)\n",
    "    df = encodage(df)\n",
    "    df = feature_engineering(df)\n",
    "    df = scaler(df)\n",
    "    X = df.drop('market-price', axis=1)\n",
    "    y = df[\"market-price\"]\n",
    "    return X, y,df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b74b3790-cf15-4f84-a74c-9ec5cc66993c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## 3.2 - Mise en forme des données pour le réseau LSTM\n",
    "\n",
    "Here we split and process data before training.\n",
    "\n",
    "LSTM layer as an input layer expects the data to be 3 dimensions, we will use 'process_data' function to split data into sequences of a fixed length (rnn_size).\n",
    "\n",
    "The neural network is expecting to have an input's shap of [batch_size, rnn_size, nb_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blockchain = pd.read_csv(os.path.join(DATADIR, \"df_blockchain.csv\"), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X,y,df=preprocessing(df_blockchain[2000:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitting(df,start,nb_split,size_test):\n",
    "    \"\"\"\n",
    "    Entries : \n",
    "        df (pandas DataFrame)\n",
    "        start (int) : jour de début de l'étude\n",
    "        nb_split (int) : represente le nombre de découpe pour la cross validation \n",
    "        size_test (int) : représente le nombre de jour du test set \n",
    "    ====================================================================================================\n",
    "    Aim : Créer une liste de X,y pour réaliser une cross validation avec le respect de la temporalité\n",
    "    ====================================================================================================\n",
    "    Outputs: \n",
    "        list_df_train --> list of df sampled train \n",
    "        list_df_test --> list of df sampled test\n",
    "    \"\"\"\n",
    "\n",
    "    tscv = TimeSeriesSplit(n_splits=nb_split, test_size=size_test)\n",
    "    list_df_train,list_df_test=[],[]\n",
    "    df=df.iloc[start:]\n",
    "    for train_index, test_index in tscv.split(df):\n",
    "        print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "        df_train=df.iloc[train_index[0]:train_index[-1]]\n",
    "        df_test=df.iloc[test_index[0]:test_index[-1]]\n",
    "        list_df_test.append(df_test)\n",
    "        list_df_train.append(df_train)\n",
    "    return list_df_train,list_df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns=df.columns\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data(data, rnn_size=3, target_id=0, columns_size=len(columns)-1):## -1 car on ne veut pas market price dans X \n",
    "    \"\"\"\n",
    "    Entries:\n",
    "    data Pandas DataFrame to be processed for RNN \n",
    "    rnn_size (int) --> nombre de récurrence d'étude pour le RNN \n",
    "    target_id (int) --> nombre de la colonne objectif \n",
    "\n",
    "    ====================================================================================================\n",
    "    Aim: processing X et y for RNN \n",
    "    ====================================================================================================\n",
    "    Outputs:\n",
    "    X_train (np.array)\n",
    "    y_train (np.array)\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "    for i in range(len(data)-rnn_size):\n",
    "        X.append(data.iloc[i:i+rnn_size,1:])# pas de market price dans X \n",
    "        y.append(data.iloc[i+rnn_size,0])\n",
    "    return np.array(X).astype(np.float32).reshape((-1,rnn_size,columns_size)), np.array(y).astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_test_train(list_df_train, list_df_test,rnn_size=3):\n",
    "    \"\"\"\n",
    "    Entries:\n",
    "    list_df_train (list) : liste contenant les DataFrame du jeu d'entraînement de cross-validation\n",
    "    list_df_test (list) : liste contenant les DataFrame du jeu de test de cross-validation\n",
    "    ====================================================================================================\n",
    "    Aim: Génère 4 listes, une contenant les variables explicatives d'entraînement, une contenant la variable à expliquer d'entraînement et pareil pour le test\n",
    "    ====================================================================================================\n",
    "    Outputs:\n",
    "    list_X_train (list)\n",
    "    list_X_test (list)\n",
    "    list_y_train (list)\n",
    "    list_y_test (list)\n",
    "    \"\"\"\n",
    "    list_X_train, list_X_test, list_y_train, list_y_test = [], [], [], []\n",
    "    for df_train in list_df_train:\n",
    "        X_train, y_train = process_data(df_train,rnn_size=rnn_size)\n",
    "        list_X_train.append(X_train)\n",
    "        list_y_train.append(y_train)\n",
    "    for df_test in list_df_test:\n",
    "        X_test, y_test = process_data(df_test,rnn_size=rnn_size)\n",
    "        list_X_test.append(X_test)\n",
    "        list_y_test.append(y_test)\n",
    "    return list_X_train, list_X_test, list_y_train, list_y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "list_df_train,list_df_test=splitting(df,0,4,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fc2c7dca-1312-497b-bbd8-16453b64bea0",
     "showTitle": false,
     "title": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "look_back=20\n",
    "list_X_train, list_X_test, list_y_train, list_y_test=split_test_train(list_df_train, list_df_test,rnn_size=look_back)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous créerons un list_X_val et list_y_val par la suite \n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 - Régressions linéaire classique et régression Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Lasso and Linear Regression to see if simple method perform well and the importance of features which has been compared with random forest features importance estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.1 - Approche naïve\n",
    "Comme suggéré durant la phase de questions de la présentation initiale (mercredi), nous avons mis en place une approche naïve qui consiste à prédire la valeur du Bitcoin au jour J en utilisant la moyenne glissante des 20 journées précédentes. Cette implémentation nous permettre ensuite de comparer cette approche archaïque aux modèles plus élaborés."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définition des jeux d'entraînement, de validation et de test\n",
    "X_train = X.iloc[:1400, :].values\n",
    "X_test = X.iloc[1401:, :].values\n",
    "y_train = y[:1400].values\n",
    "y_test = y[1401:].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(1000, 1400, 400), y_train[-400:], \"r--\")\n",
    "plt.plot(np.linspace(1401, 1540, 139), y_test, \"m--\")\n",
    "plt.xlabel(\"Jours\")\n",
    "plt.ylabel(\"Prix (USD)\")\n",
    "plt.grid()\n",
    "plt.title(\"Prix de la période d'entraînement (rouge) et de test (violet)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X_train shape : {}'.format(X_train.shape))\n",
    "print('y_train shape : {}'.format(y_train.shape))\n",
    "print('X_test shape : {}'.format(X_test.shape))\n",
    "print('y_test shape : {}'.format(y_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = X_train.shape[0]\n",
    "n_test = X_test.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardisation des données\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_n = scaler.transform(X_train)\n",
    "X_test_n = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# méthode naïve : on calcule la moyenne des prix des 20 jours précédents pour prédire le prix au jour J\n",
    "prices = df[\"market-price\"]\n",
    "index = 0\n",
    "pred_prices = []\n",
    "for new_price in y_test:\n",
    "    if index == 0:\n",
    "        pred_price = y_train[-20 + index:].mean()\n",
    "    else:\n",
    "        pred_price = np.concatenate((y_train[-20 + index:], y_test[0:index]), axis = 0).mean()\n",
    "    pred_prices.append(pred_price)\n",
    "    index +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSE = 0\n",
    "N = len(y_test)\n",
    "for val in range(N):\n",
    "    MSE += 1/N * (pred_prices[val] - y_test[val])**2\n",
    "RMSE = round(np.sqrt(MSE), 2)\n",
    "print(\"La valeur RMSE de la prédiction naïve est de\", RMSE, \"$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le score est mauvais, on en déduit qu'un modèle plus raffiné est nécessaire pour obtenir une bonne prédiction du prix du Bitcoin.\n",
    "### 4.1.2 - Régression linéaire classique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les variables explicatives (features) sont déjà sélectionnées en fonction de leurs caractériques (elles sont passées par la fonction \"preprocessing\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construction et fit du modèle\n",
    "lin_reg = LinearRegression().fit(X_train_n, y_train)\n",
    "\n",
    "# prédiction \n",
    "y_train_hat = lin_reg.predict(X_train_n)\n",
    "y_test_hat = lin_reg.predict(X_test_n)\n",
    "\n",
    "# erreur d'entraînement et de test\n",
    "err_train = mean_squared_error(y_train_hat, y_train, squared = False) # valeur RMSE\n",
    "err_test = mean_squared_error(y_test_hat, y_test, squared = False)\n",
    "print(\"L'erreur d'entraînement est de {:.2f}$ et celle de test est de {:.2f}$.\".format(err_train, err_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 4))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test, y_test_hat, label = 'Prédictions vs. réel (jeu de test)')\n",
    "plt.plot([min(y_test),max(y_test)], [min(y_test), max(y_test)], \"r--\")\n",
    "plt.legend()\n",
    "plt.title(\"Régression linéaire classique (jeu de test)\")\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_train, y_train_hat, label = \"Prédictions vs. réel (jeu d'entraînement)\")\n",
    "plt.plot([min(y_train),max(y_train)], [min(y_train), max(y_train)], \"r--\") # ligne rouge pointillée représente la prédiction parfaite\n",
    "plt.legend()\n",
    "plt.title(\"Régression linéaire classique (jeu d'entraînement)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Malgré la feature engineering, le score n'est pas très bon. Peut-on faire mieux avec une régression Lasso ? LA regression va nous informer sur l'importance des paramètres parmis ceux retenus, nous étudirions également par la suite si Lasso sur la dataset brut nous conduit à la même selection de modèle. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.3 - Regression Lasso sur les données pré-processées"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"alpha\": np.logspace(0, 2, 20)}\n",
    "lasso = Lasso()\n",
    "cv = GridSearchCV(lasso, param_grid = param_grid, cv = 3) # cross-validation\n",
    "cv.fit(X_train_n, y_train)\n",
    "\n",
    "# représentation des coefficients du meilleur estimateur\n",
    "print(\"Nombre de coefficients non-nuls :\", (cv.best_estimator_.coef_ != 0).sum())\n",
    "print(\"Meilleure valeur pour alpha :\", cv.best_params_['alpha'])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(cv.best_estimator_.coef_, \"rx\")\n",
    "ax.set_title('Coefficients de la régression Lasso')\n",
    "plt.xlabel(\"Variables explicatives\")\n",
    "plt.ylabel(\"Coefficients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On peut donc voir que les 3 variables explicatives les plus importantes, sont  la 7, la 11 et la 12 (respectivement \"cost-per-transaction\", \"year\" et \"month\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_train_lasso = mean_squared_error(cv.best_estimator_.predict(X_train_n), y_train, squared = False)\n",
    "err_test_lasso = mean_squared_error(cv.best_estimator_.predict(X_test_n), y_test, squared = False)\n",
    "y_pred = cv.best_estimator_.predict(X_test_n)\n",
    "y_train_pred = cv.best_estimator_.predict(X_train_n)\n",
    "\n",
    "print(\"L'erreur d'entraînement est de {:.2f}$ et celle de test est de {:.2f}$.\".format(err_train_lasso, err_test_lasso))\n",
    "\n",
    "plt.figure(figsize = (14, 4))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test, y_pred, label = 'Prédictions vs. réel (jeu de test)')\n",
    "plt.plot([min(y_test),max(y_test)], [min(y_test), max(y_test)], \"r--\")\n",
    "plt.legend()\n",
    "plt.title(\"Régression linéaire du Lasso sur le jeu de test\")\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_train, y_train_pred, label = \"Prédictions vs. réel (jeu d'entraînement)\")\n",
    "plt.plot([min(y_train),max(y_train)], [min(y_train), max(y_train)], \"r--\") # ligne rouge pointillée représente la prédiction parfaite\n",
    "plt.legend()\n",
    "plt.title(\"Régression linéaire classique (jeu d'entraînement)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une explication possible est la rupture de tendance observée sur le graphe du prix en USD du Bitcoin plus haut (graphe bicolore). En effet, si on représente tous les prix sur un même graphe, on remarque une hétérogénéité de leur distribution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 4))\n",
    "plt.scatter(y_test, y_pred, label = 'Prédictions vs. réel (jeu de test)')\n",
    "plt.scatter(y_train, y_train_pred, label = \"Prédictions vs. réel (jeu d'entraînement)\")\n",
    "plt.plot([min(y_train),max(y_train)], [min(y_train), max(y_train)], \"g--\") # ligne rouge pointillée représente la prédiction parfaite\n",
    "plt.plot([min(y_test),max(y_test)], [min(y_test), max(y_test)], \"r--\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Régression linéaire du Lasso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1.4 - Régression Lasso sur les données brutes (directement depuis le .csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le modèle Lasso possède un hyperparamètre $\\alpha$ qu'il faut fixer. Pour trouver la meilleure valeur possible pour $\\alpha$, on effectue une validation croisée sur le jeu d'entraînement en utilisant l'objet GridSearchCV de sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_blockchain.dropna(axis=0,inplace=True)\n",
    "X=df_blockchain.drop([\"market-price\",\"Date\"],axis=1)\n",
    "y=df_blockchain[\"market-price\"]\n",
    "X_train = X.iloc[:1400, :].values\n",
    "X_test = X.iloc[1401:, :].values\n",
    "y_train = y[:1400].values\n",
    "y_test = y[1401:].values\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train_n = scaler.transform(X_train)\n",
    "X_test_n = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\"alpha\": np.logspace(0, 2, 20)}\n",
    "lasso = Lasso()\n",
    "cv = GridSearchCV(lasso, param_grid = param_grid, cv = 3) # cross-validation\n",
    "cv.fit(X_train_n, y_train)\n",
    "\n",
    "# représentation des coefficients du meilleur estimateur\n",
    "print(\"Nombre de coefficients non-nuls :\", (cv.best_estimator_.coef_ != 0).sum())\n",
    "print(\"Meilleure valeur pour alpha :\", cv.best_params_['alpha'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.plot(cv.best_estimator_.coef_,\"rx\")\n",
    "ax.set_title('Coefficients de la régression Lasso')\n",
    "plt.xlabel(\"Variables explicatives\")\n",
    "plt.ylabel(\"Coefficients\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ceci vient confirmer notre étude passée. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "err_train_lasso = mean_squared_error(cv.best_estimator_.predict(X_train_n), y_train, squared = False)\n",
    "err_test_lasso = mean_squared_error(cv.best_estimator_.predict(X_test_n), y_test, squared = False)\n",
    "y_pred = cv.best_estimator_.predict(X_test_n)\n",
    "y_train_pred = cv.best_estimator_.predict(X_train_n)\n",
    "\n",
    "print(\"L'erreur d'entraînement est de {:.2f}$ et celle de test est de {:.2f}$.\".format(err_train_lasso, err_test_lasso))\n",
    "\n",
    "plt.figure(figsize = (14, 4))\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_test, y_pred, label = 'Prédictions vs. réel (jeu de test)')\n",
    "plt.plot([min(y_test),max(y_test)], [min(y_test), max(y_test)], \"r--\")\n",
    "plt.legend()\n",
    "plt.title(\"Régression linéaire du Lasso sur le jeu de test\")\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.scatter(y_train, y_train_pred, label = \"Prédictions vs. réel (jeu d'entraînement)\")\n",
    "plt.plot([min(y_train),max(y_train)], [min(y_train), max(y_train)], \"r--\") # ligne rouge pointillée représente la prédiction parfaite\n",
    "plt.legend()\n",
    "plt.title(\"Régression linéaire classique (jeu d'entraînement)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Une explication possible est la rupture de tendance observée sur le graphe du prix en USD du Bitcoin plus haut (graphe bicolore). En effet, si on représente tous les prix sur un même graphe, on remarque une hétérogénéité de leur distribution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (14, 4))\n",
    "plt.scatter(y_test, y_pred, label = 'Prédictions vs. réel (jeu de test)')\n",
    "plt.scatter(y_train, y_train_pred, label = \"Prédictions vs. réel (jeu d'entraînement)\")\n",
    "plt.plot([min(y_train),max(y_train)], [min(y_train), max(y_train)], \"g--\") # ligne rouge pointillée représente la prédiction parfaite\n",
    "plt.plot([min(y_test),max(y_test)], [min(y_test), max(y_test)], \"r--\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title(\"Régression linéaire du Lasso\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 - Modèles de Deep Learning (réseaux de neurones) pour la prédiction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aim : to predict the 30 last day of BTC\n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall of Deep learning in a noteshell : \n",
    "* **Metric** values are recorded at the end of each epoch on the training dataset. If a validation dataset is also provided, then the metric recorded is also calculated for the validation dataset.\n",
    "\n",
    "\n",
    "* A **loss function** is used to train your model. A metric is used to evaluate your model. A loss function is used during the learning process. A metric is used after the learning process. \n",
    "\n",
    "\n",
    "* An **optimizer** is a method or algorithm to update the various parameters that can reduce the loss in much less effort. \n",
    "\n",
    "\n",
    "* The **learning rate** is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. This is a key parameters. We have to take the half of the learning that makes the model diverging. In other word, we have te start with a relativly big learning rate and decrease manually by dividing by 2 for example, till it stop to diverge.\n",
    "\n",
    "* **Drop** : dropout refers to ignoring units (i.e. neurons) during the training phase of certain set of neurons which is chosen at random. It aims to reduce risk of overfitting. \n",
    "\n",
    "\n",
    "* **Epochs** : indicates the number of passes of the entire training dataset the machine learning algorithm has completed\n",
    "\n",
    "\n",
    "* **Early stopping** : Early stopping is designed to monitor the generalization error of one model and stop training when generalization error begins to degrade. In a way it means you will not do epochs for nothing. \n",
    "\n",
    "\n",
    "* **Batch size** :  refers to the number of training examples utilized in one iteration. Small batch size increase risk of overfitting because the weights are computed for each batch. Therefore there is a comprise to do. \n",
    "\n",
    "\n",
    "* **Verbose** : degree of detail when executing a model\n",
    "\n",
    "\n",
    "* **RNN** : Recurrent Neural Network, are a class of neural networks that allow previous outputs to be used as inputs while having hidden states.\n",
    "\n",
    "\n",
    "* **CNN** : Convolutional Neural Network \n",
    "\n",
    "* **Batch normalization** : In a 2015 paper, Sergey Ioffe and Christian Szegedy proposed a technique called Batch Normalization (BN) to address the vanishing/exploding gradients problems. The technique consists of adding an operation in the model just before or after the activation function of each hidden layer, simply zero-centering and normalizing each input, then scaling and shifting the result using two new parameter vectors per layer: one for scaling, the other for shifting. In other words, this operation lets the model learn the optimal scale and mean of each of the layer’s inputs. \n",
    "\n",
    "* **Initializer** :  define the way to set the initial random weights of Keras layers. It is a way to avoid vanishing or exploding gradient. There are many intializer but they have to be choose accordingly to the activation function : \n",
    "        Glorot uniform or normal for : None, Tanh, Logistic, Softmax\n",
    "        He uniform or normal for : ReLU & variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*  **training dataset**  is a set of examples used to fit the parameters (e.g. weights of connections between neurons in artificial neural networks) of the model.\n",
    "* ** test dataset ** is a dataset used to provide an unbiased evaluation of a final model fit on the training dataset.If the data in the test dataset has never been used in training (for example in cross-validation), the test dataset is also called a holdout dataset. The term \"validation set\" is sometimes used instead of \"test set\" in some literature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN \n",
    "| Advantages | Drawbacks |\n",
    "|:-:|:-:|\n",
    "| Model size not increasing with size of input | Computation being slow |\n",
    "| Computation takes into account historical information| Difficulty of accessing information from a long time ago |\n",
    "| Weights are shared across time | Cannot consider any future input for the current state|\n",
    "| Possibility of processing input of any length |\n",
    "\n",
    "Therefore we can easly understand why the RNN are very good for time series forecasting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gated Recurrent Unit (GRU) and Long Short-Term Memory units (LSTM) deal with the vanishing gradient problem encountered by traditional RNNs, with LSTM being a generalization of GRU. \n",
    "* **\n",
    "### 1. LSTM standing for Long Short Term Memory (RNN type):\n",
    "\n",
    "Bi-directional LSTM is a variant : \n",
    "LSTM in its core, preserves information from inputs that has already passed through it using the hidden state.\n",
    "\n",
    "Unidirectional LSTM only preserves information of the past because the only inputs it has seen are from the past.\n",
    "\n",
    "Using bidirectional will run your inputs in two ways, one from past to future and one from future to past and what differs this approach from unidirectional is that in the LSTM that runs backwards you preserve information from the future and using the two hidden states combined you are able in any point in time to preserve information from both past and future.\n",
    "\n",
    "We tried out this kind of RNN (BidirLSTM) but after few reflexions it was not a good idea because of temporality importance. \n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Classical ANN : \n",
    "testing for many differents architecture : We have to be careful about the number of parameters because we only have a time series with 1500 days and about 16 columns, which is very few compared to what is used to be in DL. Therefore, we have limited our number of hidden layers and the number of neurons per layer. \n",
    "\n",
    "* To determine the number of layers : increase gradually the number of layer till overfitting. \n",
    "\n",
    "\n",
    "* The number of neurons per layers is determine by the problem here the number of data is limited. Moreover a \"cone\" architechture is more commune.\n",
    "\n",
    "* To determine the learning rate : We have to take the half of the learning that makes the model diverging. In other word, we have te start with a relativly big learning rate and decrease manually by dividing by 2 for example, till it stop to diverge. \n",
    "\n",
    "* To determine batch_size and optimizer we have done some benchmark to see what was the best configuration. \n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. CNN + LSTM : \n",
    "Here, we have stacked one layer of 1D convolution, 1 layer of LSTM and one Dense. \n",
    "Clearly this ANN is more complicated than the other two, it requires much more epochs to be trained even if there is the amount of parameters. \n",
    "Concerning hyperparameters we have choosen a random search to fine tune our model. \n",
    "* **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More generally we always started with very simple model and then we have augmented the complexity to get better results. To be confident in our choices, we have to keep in mind that we only have 1500 datas with 16 features, so the number of parameters to be optimized must not be to much large. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly let's define what a **GridSearchCV** is. This is an Exhaustive search over specified parameter values for an estimator. GridSearchCV implements a “fit” and a “score” method. It also implements “score_samples”, “predict”, “predict_proba”, “decision_function”, “transform” and “inverse_transform” if they are implemented in the estimator used.The parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid. \n",
    "We have used **Randomized Search** for time efficiency. In contrast to GridSearchCV, not all parameter values are tried out, but rather a fixed number of parameter settings is sampled from the specified distributions. The number of parameter settings that are tried is given by n_iter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **\n",
    "## Optimizer : \n",
    "* **RMSprop** : The gist of RMSprop is to:Maintain a moving (discounted) average of the square of gradients Divide the gradient by the root of this average This implementation of RMSprop uses plain momentum, not Nesterov momentum. The centered version additionally maintains a moving average of the gradients, and uses that average to estimate the variance. In other words, it reduce the moving in one direction to prefer the one which will hep to get a faster convergence.\n",
    "* **Adam** : Adam optimization is a stochastic gradient descent method that is based on adaptive estimation of first-order and second-order moments. According to Kingma et al., 2014, the method is \"computationally efficient, has little memory requirement, invariant to diagonal rescaling of gradients, and is well suited for problems that are large in terms of data/parameters\".\n",
    "There are so many other Optimizer available with keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def root_mean_squared_error(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    y_true (list or numpy.Array) : vraies valeurs du prix du Bitcoin sur la fenêtre temporelle de test\n",
    "    y_pred (list or numyp.Array) : valeurs de prix prédites par l'algorithme\n",
    "    Aim:\n",
    "    Retourne la racine de l'erreur quadratique moyenne, une métrique permettant de juger de la performance de l'algorithme\n",
    "    Output:\n",
    "    rmse (float) : racine de l'erreur quadratique moyenne\n",
    "    \"\"\"\n",
    "    return K.sqrt(K.mean(K.square(y_pred - y_true))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_LSTM(opti=\"RMSprop\",ls=35,Den=10,learning_r=0.02):\n",
    "\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    opti (string) : optimiseur à considérer pour l'entraînement du réseau de neurones. Les valeurs supportées sont : \"RMSprop\" et \"Adam\".\n",
    "    ls (int) : nombre de neurones dans la couche LSTM\n",
    "    Den (int) : nombre de neurones dans la couche cachée Dense\n",
    "    learning_r (float) : vitesse d'apprentissage du réseau\n",
    "\n",
    "    Aim:\n",
    "    Créer un modèle de réseau de neurones paramétrables par l'optimiseur, le nombre de neurones des couches principales et la vitesse d'apprentissage.\n",
    "\n",
    "    Output:\n",
    "    model (keras.models) : modèle LSTM à une couche dense cachée et une couche Dropout\n",
    "    \"\"\"\n",
    "    \n",
    "    if not isinstance(opti, str):\n",
    "            raise Exception(\n",
    "                \" opti must be a string which represents optimizer : Adam or RMSprop\")\n",
    "    if not isinstance(ls, int):\n",
    "            raise Exception(\n",
    "                \" ls must be an int which represents number of unites for LSTM\")\n",
    "    if not isinstance(Den, int):\n",
    "            raise Exception(\n",
    "                \" Den must be an int which represents number of neurons of dense\")\n",
    "\n",
    "    drop=0.3\n",
    "    initHe= HeNormal()\n",
    "    initGlo= GlorotNormal()\n",
    "    model = keras.Sequential()\n",
    "    bias=True\n",
    "    model.add(LSTM(ls,input_shape=(look_back, len(columns)-1),dropout=drop,kernel_initializer=initGlo))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(Den,use_bias=bias))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(1))\n",
    "\n",
    "    if opti==\"RMSprop\":\n",
    "        opt = keras.optimizers.RMSprop(learning_rate=learning_r,\n",
    "        rho=0.9,\n",
    "        momentum=0.0,\n",
    "        epsilon=1e-07,\n",
    "        centered=False,\n",
    "        name=\"RMSprop\")## centered peut-être à mettre en True \n",
    "        \n",
    "    if opti==\"Adam\":\n",
    "        opt =keras.optimizers.Adam(\n",
    "        learning_rate=learing_r,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        amsgrad=False,\n",
    "        name=\"Adam\")\n",
    "\n",
    "    model.compile(optimizer = opt, loss = root_mean_squared_error,metrics = [keras.metrics.RootMeanSquaredError()])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_CNN_RNN(opti=\"RSMprop\",filtre=3,kernel=3,ls=5,learning_r=0.15):\n",
    "\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    opti (string) : optimiseur à considérer pour l'entraînement du réseau de neurones. Les valeurs supportées sont : \"RMSprop\" et \"Adam\".\n",
    "    filtre (int) : dimension de l'espace de sortie\n",
    "    kernel (int) : dimension du noyau de convolution 1D\n",
    "    ls (int) : nombre de neurones dans la couche LSTM\n",
    "    learning_r (float) : vitesse d'apprentissage du réseau\n",
    "\n",
    "    Aim:\n",
    "    Créer un modèle de réseau de neurones paramétrables par l'optimiseur, le nombre de neurones des couches LSTM et Dense, la vitesse d'apprentissage et\n",
    "    des paramètres de convolution (dimension du noyau et nombre de filtres en sortie)\n",
    "\n",
    "    Output:\n",
    "    model (keras.models) : modèle CNN-RNN (CNN-LSTM) à une couche de convolution, une couche LSTM, une couche dense cachée et une couche Dropout\n",
    "    \"\"\"\n",
    "    if not isinstance(opti, str):\n",
    "            raise Exception(\n",
    "                \" opti must be a string which represents optimizer : Adam or RMSprop\")\n",
    "    drop=0.3\n",
    "    model = keras.Sequential()\n",
    "    model.add(Conv1D(filters=filtre, kernel_size=kernel,\n",
    "                         strides=1, padding=\"causal\",\n",
    "                         activation=\"relu\",\n",
    "                         input_shape=(look_back, len(columns)-1),\n",
    "                         kernel_initializer='random_normal'))\n",
    "    model.add(LSTM(ls, activation=\"tanh\", return_sequences=False))\n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Dense(1, activation=\"relu\"))## sortie positive de dimension 1 \n",
    "\n",
    "    if opti==\"RMSprop\":\n",
    "        opt = keras.optimizers.RMSprop(learning_rate=learning_r,\n",
    "        rho=0.9,\n",
    "        momentum=0.0,\n",
    "        epsilon=1e-07,\n",
    "        centered=False,\n",
    "        name=\"RMSprop\")\n",
    "\n",
    "    if opti==\"Adam\":\n",
    "        opt =keras.optimizers.Adam(\n",
    "        learning_rate=learning_r,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        amsgrad=False,\n",
    "        name=\"Adam\")\n",
    "\n",
    "    model.compile(optimizer =opt, loss = root_mean_squared_error,metrics = [keras.metrics.RootMeanSquaredError()])\n",
    "    model.summary()\n",
    "\n",
    "    return model          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_ANN(opti='Adam',Den_1=8,Den_2=20,Den_3=10,learning_r=0.001):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    opti (string) : optimiseur à considérer pour l'entraînement du réseau de neurones. Les valeurs supportées sont : \"RMSprop\" et \"Adam\".\n",
    "    Den_1 (int) : nombre de neurones dans la couche d'entrée Dense\n",
    "    Den_2 (int) : nombre de neurones dans la 1ère couche cachée Dense\n",
    "    Den_3 (int) : nombre de neurones dans la 2nde couche cachée Dense\n",
    "    learning_r (float) : vitesse d'apprentissage du réseau\n",
    "\n",
    "    Aim:\n",
    "    Créer un modèle de réseau de neurones paramétrables par l'optimiseur, les nombres de neurones des couches Dense et la vitesse d'apprentissage.\n",
    "\n",
    "    Output:\n",
    "    model (keras.models) : modèle neuronal classique composée de couches Dense et Dropout\n",
    "    \"\"\"\n",
    "    if not isinstance(opti, str):\n",
    "            raise Exception(\n",
    "                \" opti must be a string which represents optimizer : Adam or RMSprop\")\n",
    "    if not isinstance(Den_1, int):\n",
    "            raise Exception(\n",
    "                \" Den must be an int which represents number of neurons of dense\")\n",
    "    if not isinstance(Den_2, int):\n",
    "            raise Exception(\n",
    "                \" Den must be an int which represents number of neurons of dense\")\n",
    "    if not isinstance(Den_3, int):\n",
    "            raise Exception(\n",
    "                \" Den must be an int which represents number of neurons of dense\")\n",
    "\n",
    "    drop=0.3\n",
    "    model = keras.Sequential()\n",
    "    bias=True\n",
    "    model.add(Dense(Den_1,use_bias=bias,input_shape=(look_back, len(columns)-1)))\n",
    "    \n",
    "    model.add(Dropout(drop))\n",
    "    model.add(Flatten()) ## Attention \"explosion\" dimension car on flatten i.e. on repasse en dim 1 mais on multiplie la dim de l'espace latent de sortie précécent par le nombre de features = 16 \n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dense(Den_2,use_bias=bias))\n",
    "    model.add(Dropout(drop))\n",
    "    # model.add(BatchNormalization())\n",
    "    \n",
    "    model.add(Dense(Den_3,use_bias=bias))\n",
    "    model.add(Dropout(drop))\n",
    "    # model.add(BatchNormalization())\n",
    "    model.add(Dense(1,use_bias=bias))\n",
    "    if opti==\"RMSprop\":\n",
    "        opt = keras.optimizers.RMSprop(learning_rate=learning_r,\n",
    "        rho=0.9,\n",
    "        momentum=0.0,\n",
    "        epsilon=1e-07,\n",
    "        centered=False,\n",
    "        name=\"RMSprop\")## centered peut-être à mettre en True \n",
    "\n",
    "    if opti==\"Adam\":\n",
    "        opt =keras.optimizers.Adam(\n",
    "        learning_rate=learning_r,\n",
    "        beta_1=0.9,\n",
    "        beta_2=0.999,\n",
    "        epsilon=1e-07,\n",
    "        amsgrad=False,\n",
    "        name=\"Adam\")\n",
    "\n",
    "    model.compile(optimizer = opt, loss = root_mean_squared_error,metrics = [keras.metrics.RootMeanSquaredError()])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error(models):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    models (list de keras.models) : liste de modèles issus d'une validation croisée dont on veut le        score RMSE moyen (sur le jeu de test)\n",
    "\n",
    "    Aim:\n",
    "    Retourner le score RMSE d'une liste de modèles entraînés par cross-validation\n",
    "\n",
    "    Output:\n",
    "    mean_RMSE = np.mean(rms_list) (float) : score RMSE moyen de la liste de modèles\n",
    "    \"\"\"\n",
    "    if not isinstance(models, list):\n",
    "            raise Exception(\n",
    "                \" models must be a list which represents cross validated models\")\n",
    "    rms_list=[]\n",
    "    for p in range(len(models)):\n",
    "        y_pred=[]\n",
    "        for i in range(len(list_X_test)):\n",
    "            pred=models[p].predict(list_X_test[i])\n",
    "            for j in range(len(pred)):\n",
    "                y_pred.append(pred[j])\n",
    "        rms = mean_squared_error(np.ravel(list_y_test), y_pred, squared=False)\n",
    "        rms_list.append(rms)\n",
    "    \n",
    "    return np.mean(rms_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def training(type_ANN,opti,batch=32,nb_epochs=150):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    type_ANN (string) : type d'Artifical Neural Network (ANN) que l'on souhaite entraîner\n",
    "    opti (string) : nom de l'optimiseur à utiliser pour l'entraînement. Les valeurs supportées sont : \"RMSprop\" et \"Adam\".\n",
    "    batch (int) : taille des batchs à donner en entrée du réseau de neurones\n",
    "    nb_epochs : longueur (en itérations) des époques d'entraînement du réseau\n",
    "\n",
    "    Aim:\n",
    "    Entraîner le modèle spécifié par validation croisée en mettant en place un Early Stopping (arrêt de l'entraînement si des signes d'overfitting apparaissent).\n",
    "    Stocker les valeurs des erreurs d'entraînement et de test pour représentation.\n",
    "\n",
    "    Output:\n",
    "    loss (list de float) : perte RMSE du modèle sur le jeu d'entraînement au fil des époques\n",
    "    loss_val (list de float) : perte RMSE du modèle sur le jeu de validation au fil des époques\n",
    "    models (list de keras.models) : modèles entraînés par validation croisée\n",
    "    erreur (float) : perte RMSE moyenne des modèles sur le jeu de test\n",
    "    \"\"\"\n",
    "    if not isinstance(type_ANN, str):\n",
    "            raise Exception(\n",
    "                \" type_ANN must be a string which represents the type of model you want to train : ANN, RNN, CNN_RNN\")\n",
    "    if not isinstance(opti, str):\n",
    "            raise Exception(\n",
    "                \" opti must be a string which represents optimizer : Adam or RMSprop\")\n",
    "    if not isinstance(batch, int):\n",
    "            raise Exception(\n",
    "                \" Batch must be an int which represents batch_size\")\n",
    "    if not isinstance(nb_epochs, int):\n",
    "            raise Exception(\n",
    "                \" nb_epochs must be an int which represents number of epochs\")\n",
    "\n",
    "\n",
    "    loss,loss_val,weights,models=[],[],[],[]\n",
    "\n",
    "    for i in range(len(list_X_train)):\n",
    "        X_train=list_X_train[i]\n",
    "        y_train=list_y_train[i]\n",
    "        X_test=list_X_test[i]\n",
    "        y_test=list_y_test[i]\n",
    "        print(\"Cross Val no : \", i )\n",
    "        if type_ANN==\"ANN\":\n",
    "            model=create_model_ANN(opti)\n",
    "        if type_ANN==\"LSTM\":\n",
    "            model=create_model_LSTM(opti)\n",
    "        if type_ANN==\"CNN_RNN\":\n",
    "            model=create_model_CNN_RNN(opti)\n",
    "        \n",
    "        callback = EarlyStopping(monitor='val_root_mean_squared_error', patience=10)\n",
    "        history=model.fit(X_train, y_train, validation_data = (X_test, y_test), batch_size = batch, epochs = nb_epochs,verbose=False,callbacks=[callback])\n",
    "        models.append(model)\n",
    "        print(history.history.keys())\n",
    "        loss.append(history.history[\"loss\"])\n",
    "        print(\"stopped at \" ,len(history.history[\"loss\"]),\" epochs\")\n",
    "        loss_val.append(history.history['val_root_mean_squared_error'])\n",
    "    \n",
    "        weights.append(model.get_weights())\n",
    "    erreur=get_error(models)\n",
    "    return loss,loss_val,models,erreur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred(models,titre):\n",
    "\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    models (list de keras.model) : liste de modèles entraînés par validation croisée (tous du même type)\n",
    "    titre (string) : titre du graphe\n",
    "\n",
    "    Aim:\n",
    "    Représenter les prédictions des modèles en fonction des vraies valeurs sous forme de scatter plot\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "    \"\"\"\n",
    "    colormap=plt.cm.gist_ncar\n",
    "    colost=[colormap(i) for i in np.linspace(0,0.95,len(models)+1)]## len du nb de trucs à plot\n",
    "    plt.figure()\n",
    "    for p in range(len(models)):\n",
    "        y_pred=[]\n",
    "        for i in range(len(list_X_test)):\n",
    "            pred=models[p].predict(list_X_test[i])\n",
    "            for j in range(len(pred)):\n",
    "                y_pred.append(pred[j])\n",
    "        rms = mean_squared_error(np.ravel(list_y_test), y_pred, squared=False)\n",
    "        plt.scatter(np.ravel(list_y_test),y_pred,color=colost[p],alpha=0.5,label=f\"rmse : {round(rms,2)} $\") \n",
    "    plt.plot(np.linspace(30000,60000),np.linspace(30000,60000),color=\"red\") ### idendité i.e. cas idéal ou prédit = réel\n",
    "    plt.xlabel(\"True\")\n",
    "    plt.ylabel(\"Pred\")\n",
    "    plt.title(titre)\n",
    "    plt.legend()\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(loss_list,loss_val_list,title):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "    loss_list (list de float) : liste des pertes RMSE du modèle sur le jeu d'entraînement au fil des époques\n",
    "    loss_val_list (list de float) : liste des pertes RMSE du modèle sur le jeu de validation au fil des époques\n",
    "    titre (string) : titre du graphe\n",
    "\n",
    "    Aim:\n",
    "    Représenter les pertes des modèles en fonction des époques\n",
    "\n",
    "    Output:\n",
    "    None\n",
    "\"\"\"\n",
    "    for i in range(len(loss_list)):\n",
    "        plt.figure()\n",
    "        plt.plot(loss_list[i],color=\"orange\",label=\"loss\")\n",
    "        plt.plot(loss_val_list[i],color=\"blue\",label=\"val loss\")\n",
    "        plt.title(f'model {title} loss')\n",
    "        plt.ylabel('loss')\n",
    "        plt.xlabel('epoch')\n",
    "        plt.legend(loc='upper right')\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 - Etude du modèle LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_distribs = {\n",
    " \"opti\":[\"Adam\",\"RMSprop\"],\n",
    " \"ls\": [25,35,50,60],\n",
    " \"Den\": [10,20,25,30],\n",
    " \"learning_r\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "X_train=list_X_train[0]\n",
    "y_train=list_y_train[0]\n",
    "y_valid=list_y_test[0]\n",
    "X_valid=list_X_test[0]\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(create_model_LSTM)\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10,n_jobs=-1)# n_jobs pour exploiter tous les cpu \n",
    "rnd_search_cv.fit(X_train, y_train, epochs=25,\n",
    " validation_data=(X_valid, y_valid),\n",
    " callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnd_search_cv.best_params_)\n",
    "print(rnd_search_cv.best_score_)\n",
    "model_LSTM = rnd_search_cv.best_estimator_.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère alors les meilleurs hyperparamètres, que l'on va mettre comme par défault dans la fonction de création. On va ensuite chercher à regarder l'impact du batch_size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in [32,64,128]:\n",
    "    loss,loss_val,models,err=training(\"LSTM\",\"RMSprop\",batch,500)\n",
    "    print(\"RMSE erreur en $ USD\",err)\n",
    "    plot_loss(loss,loss_val,\"LSTM \")\n",
    "    plot_pred(models,\"Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En défilant la barre de droite, on observe plusieurs diagrammes de perte. Pour la première série, on voit les pertes sur les jeux d'entraînement et de validation pour un modèle à couches Dense classique en fonction de itérations de validation croisée. Au fur et à mesure des itérations, l'erreur de validation decroît pour arriver au même niveau que l'erreur d'entraînement. Le graphe de type \"scatter plot\" suivant indique la distribution des points (valeur prédite, valeur réelle), la courbe rouge représentant la prédiction idéale. On note des RMSE élevés, le meilleur score étant à 9k$. Ces simulations ont été faites pour une taille de batch de 128. La même opération est repétée pour des tailles de batch de 32 et 64 (les graphes sont à la suite). Même si les valeurs de RMSE sont toutes mauvaises, on remarque qu'une taille de 64 peut être retenue pour la suite car donne les meilleurs résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=list_X_train[-1]\n",
    "y_train=list_y_train[-1]\n",
    "y_valid=list_y_test[-1]\n",
    "X_valid=list_X_test[-1]\n",
    "\n",
    "callback = EarlyStopping(monitor='val_root_mean_squared_error', patience=10)\n",
    "t1=time.time()\n",
    "mode_lstm=create_model_LSTM()\n",
    "mode_lstm.fit(X_train, y_train, validation_data = (X_valid, y_valid), batch_size = 128, epochs = 1000,verbose=False,callbacks=[callback])# mettre le meilleur opti \n",
    "t_lstm=time.time()-t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The LSTM model was trained {} s\".format(t_lstm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_pred([mode_lstm],\"Prédiction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que la prédiction du modèle LSTM + Dense est mauvaise. En effet, le réseau ne prédit que des valeurs entre 45k$ et 55k$. Il faut par conséquent construire des modèles différents pour améliorer la métrique RMSE. On va à ce titre explorer les modèles Dense classiques et les LSTM avec une couche de convolution (CNN-LSTM)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_lstm.save(r\"..\\models\\LSTM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3 - Etude du modèle classique (couches Dense), dit ANN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_distribs = {\n",
    " \"opti\":[\"Adam\",\"RMSprop\"],\n",
    " \"Den_1\": [4,6,8],\n",
    " \"Den_2\": [20,30,50],\n",
    " \"Den_3\": [5,10,15],\n",
    " \"learning_r\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "X_train=list_X_train[0]\n",
    "y_train=list_y_train[0]\n",
    "y_valid=list_y_test[0]\n",
    "X_valid=list_X_test[0]\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(create_model_ANN)\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10,n_jobs=-1)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=50,\n",
    " validation_data=(X_valid, y_valid),\n",
    " callbacks=[keras.callbacks.EarlyStopping(patience=10)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnd_search_cv.best_params_)\n",
    "print(rnd_search_cv.best_score_)\n",
    "model_LSTM = rnd_search_cv.best_estimator_.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère alors les meilleurs hyperparamètres, que l'on va mettre comme par défault dans la fonction de création. On va ensuite chercher à regarder l'impact du batch_size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in [32,64,128]:\n",
    "    loss,loss_val,models,err=training(\"ANN\",\"Adam\",batch,500)## attention changer ADAM si c'est pas le meilleurs\n",
    "    print(\"RMSE erreur en $ USD\",err)\n",
    "    plot_loss(loss,loss_val,\"ANN \")\n",
    "    plot_pred(models,\"Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En défilant la barre de droite, on observe plusieurs diagrammes de perte. Pour la première série, on voit les pertes sur les jeux d'entraînement et de validation pour un modèle à couches Dense classique en fonction de itérations de validation croisée. Au fur et à mesure des itérations, l'erreur de validation decroît pour arriver au même niveau que l'erreur d'entraînement. Le graphe de type \"scatter plot\" suivant indique la distribution des points (valeur prédite, valeur réelle), la courbe rouge représentant la prédiction idéale. On note des RMSE élevés, le meilleur score étant à 9k$. Ces simulations ont été faites pour une taille de batch de 64. La même opération est repétée pour des tailles de batch de 32 et 128 (les graphes sont à la suite). Même si les valeurs de RMSE sont toutes mauvaises, on remarque qu'une taille de 64 peut être retenue pour la suite car donne les meilleurs résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=list_X_train[-1]\n",
    "y_train=list_y_train[-1]\n",
    "y_valid=list_y_test[-1]\n",
    "X_valid=list_X_test[-1]\n",
    "\n",
    "callback = EarlyStopping(monitor='val_root_mean_squared_error', patience=10)\n",
    "t2=time.time()\n",
    "mode_ANN=create_model_ANN()\n",
    "mode_ANN.fit(X_train, y_train, validation_data = (X_valid, y_valid), batch_size = 64, epochs = 1000,verbose=False,callbacks=[callback])# mettre le meilleur opti \n",
    "t_ann=time.time()-t2\n",
    "plot_pred([mode_ANN],\"Prédiction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"the ANN model was trained in {t_ann} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_ANN.save(r\"..\\models\\ANN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4 - Etude du modèle CNN-RNN (réseau convolutionnel récurrent, ou CNN/LSTM) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "param_distribs = {\n",
    " \"opti\":[\"Adam\",\"RMSprop\"],\n",
    " \"filtre\": [2,3,4,5,6,7,8],\n",
    " \"kernel\": [3,4,5,6],\n",
    " \"ls\":[3,4,5,6],\n",
    " \"learning_r\": reciprocal(3e-4, 3e-2),\n",
    "}\n",
    "X_train=list_X_train[0]\n",
    "y_train=list_y_train[0]\n",
    "y_valid=list_y_test[0]\n",
    "X_valid=list_X_test[0]\n",
    "keras_reg = keras.wrappers.scikit_learn.KerasRegressor(create_model_CNN_RNN)\n",
    "rnd_search_cv = RandomizedSearchCV(keras_reg, param_distribs, n_iter=10,n_jobs=-1)\n",
    "rnd_search_cv.fit(X_train, y_train, epochs=50,\n",
    " validation_data=(X_valid, y_valid),\n",
    " callbacks=[keras.callbacks.EarlyStopping(patience=10)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(rnd_search_cv.best_params_)\n",
    "print(rnd_search_cv.best_score_)\n",
    "model_CNN_RNN = rnd_search_cv.best_estimator_.model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère alors les meilleurs hyperparamètres, que l'on va mettre comme par défault dans la fonction de création. On va ensuite chercher à regarder l'impact du batch_size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in [32,64,128]:\n",
    "    loss,loss_val,models,err=training(\"CNN_RNN\",\"RMSprop\",batch,500)\n",
    "    print(\"RMSE erreur en $ USD\",err)\n",
    "    plot_loss(loss,loss_val,\"CNN_RNN \")\n",
    "    plot_pred(models,\"Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En défilant la barre de droite, on observe plusieurs diagrammes de perte. Pour la première série, on voit les pertes sur les jeux d'entraînement et de validation pour un modèle à couches Dense classique en fonction de itérations de validation croisée. Au fur et à mesure des itérations, l'erreur de validation decroît pour arriver au même niveau que l'erreur d'entraînement. Ici il y a clairement de l'underfitting on pourrait utiliser des méthodes pour augmenter artificellement le jeu de données. Le graphe de type \"scatter plot\" suivant indique la distribution des points (valeur prédite, valeur réelle), la courbe rouge représentant la prédiction idéale. On note des RMSE élevés, le meilleur score étant à 9k$. Ces simulations ont été faites pour une taille de batch de 64. La même opération est repétée pour des tailles de batch de 32 et 128 (les graphes sont à la suite). Même si les valeurs de RMSE sont toutes mauvaises, on remarque qu'une taille de 64 peut être retenue pour la suite car donne les meilleurs résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=list_X_train[-1]\n",
    "y_train=list_y_train[-1]\n",
    "y_valid=list_y_test[-1]\n",
    "X_valid=list_X_test[-1]\n",
    "\n",
    "callback = EarlyStopping(monitor='val_root_mean_squared_error', patience=10)\n",
    "t2=time.time()\n",
    "mode_CNN_RNN=create_model_CNN_RNN()\n",
    "mode_CNN_RNN.fit(X_train, y_train, validation_data = (X_valid, y_valid), batch_size =64 , epochs = 1000,verbose=False,callbacks=[callback])# mettre le meilleur opti \n",
    "t_cnn=time.time()-t2\n",
    "plot_pred([mode_CNN_RNN],\"Prédiction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"the ANN model was trained in {t_cnn} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode_CNN_RNN.save(r\"..\\models\\RNN_CNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 - Conclusion \n",
    "We have tried out to select some models but with more time we are convinced that we could have builded much more efficient models. \n",
    "To sum up our results : \n",
    "\n",
    "| Model | RMSE | Time | Numbers of params|\n",
    "|:-:|:-:|:-:|:-:|\n",
    "| LSTM | 8854 $ USD |17.82 s  | 7 511 |\n",
    "| ANN Dense| 10616 $ USD  |4.18 s | 3 569 |\n",
    "| CNN + LSTM  | 42 172 $ USD | 197.13 s | 324 |\n",
    "\n",
    "We can say that was definitly a difficult subject with the fact that the end of the time serie did not happend in the past. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7 - Sources : \n",
    "* [Batch size issue ] (https://machinelearningmastery.com/use-different-batch-sizes-training-predicting-python-keras/?fbclid=IwAR3zjCnpr8tC4nYN4q2m1LXbyteFECLqfLgBupzsz5x_FWdQN76Hu3Z7Hck)\n",
    "* [feature engineering random forest] (https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances.html)\n",
    "* [RNN cheatsheet](https://stanford.edu/~shervine/teaching/cs-230/cheatsheet-recurrent-neural-networks)\n",
    "* [ RmsProp basics ](https://towardsdatascience.com/a-look-at-gradient-descent-and-rmsprop-optimizers-f77d483ef08b)\n",
    "* [ RMSprop keras](https://keras.io/api/optimizers/rmsprop/)\n",
    "* [SGD](https://scikit-learn.org/stable/modules/sgd.html)\n",
    "* [Loss functions & time series] (http://www.faculty.ucr.edu/~taelee/paper/lossfunctions.pdf)\n",
    "* [Hand on machine learning with scikit-learn, keras & tensorflow]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8 - Annexe "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.1 - Etude du modèle LSTM Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_features = len(columns) - 1 # nombre de variables explicatives\n",
    "batch_size = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on construit un modèle de réseau de neurones récurrent de type LSTM (Long Short-Term Memory)\n",
    "def create_simple_LSTM_model():\n",
    "    # Build the model\n",
    "    model = keras.Sequential()\n",
    "    model.add(LSTM(1,\n",
    "                batch_input_shape = (None, look_back, number_features)))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = \"adam\", loss = 'mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_simple_LSTM_model()\n",
    "#loss = [] # valeur de la fonction de perte sur le jeu de test\n",
    "#loss_val = [] # valeur de la fonction de perte sur le jeu d'entraînement\n",
    "i = 1\n",
    "X_train = list_X_train[i]\n",
    "y_train = list_y_train[i]    \n",
    "X_test = list_X_test[i]\n",
    "y_test = list_y_test[i]\n",
    "history = model.fit(X_train,y_train,validation_data = (X_test, y_test), batch_size = batch_size, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.linspace(1, 10, 10)\n",
    "losses = history.history[\"loss\"] # perte du jeu d'entraînement\n",
    "val_losses = history.history[\"val_loss\"] # perte du jeu de test\n",
    "plt.plot(epochs, losses, \"xb\")\n",
    "plt.plot(epochs, val_losses, \"xr\")\n",
    "plt.title(\"Perte en fonction des époques\")\n",
    "plt.xlabel(\"Epoques\")\n",
    "plt.ylabel(\"Perte (Mean Squared Error)\")\n",
    "plt.legend([\"Entraînement\", \"Test\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred[:, -1]\n",
    "print(y_pred, y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(\"La RMSE du modèle est :\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8.2 - Etude du modèle LSTM simple avec couche Dropout\n",
    "On ajoute un \"dropout\" au réseau LSTM simple. Ce paramètre permet de fixer une certaine proportion des poids pour une itération (ils ne seront pas modifiés par back-propagation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on construit un modèle de réseau de neurones récurrent de type LSTM (Long Short-Term Memory)\n",
    "def create_dropout_LSTM_model(dropout):\n",
    "    # Build the model\n",
    "    model = keras.Sequential()\n",
    "    model.add(LSTM(1,\n",
    "                batch_input_shape = (None, look_back, number_features),\n",
    "                dropout = dropout))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer = \"adam\", loss = 'mse')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "i=1\n",
    "X_train = list_X_train[i]\n",
    "y_train = list_y_train[i]    \n",
    "X_test = list_X_test[i]\n",
    "y_test = list_y_test[i]\n",
    "history = model.fit(X_train,y_train,\n",
    "                validation_data = (X_test, y_test),\n",
    "                batch_size = batch_size,\n",
    "                epochs = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.linspace(1, 300, 300)\n",
    "losses = history.history[\"loss\"] # perte du jeu d'entraînement\n",
    "val_losses = history.history[\"val_loss\"] # perte du jeu de test\n",
    "plt.plot(epochs, losses, \"xb\")\n",
    "plt.plot(epochs, val_losses, \"xr\")\n",
    "plt.title(\"Perte en fonction des époques\")\n",
    "plt.xlabel(\"Epoques\")\n",
    "plt.ylabel(\"Perte (Mean Squared Error)\")\n",
    "plt.legend([\"Entraînement\", \"Test\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred[:, -1]\n",
    "print(y_pred, y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred)\n",
    "plt.plot(np.linspace(np.min(y_test),np.max(y_test), len(y_pred)),\n",
    "        np.linspace(np.min(y_test),np.max(y_test), len(y_pred)),\n",
    "        color = \"r\")\n",
    "plt.title(\"Prédictions du réseau LSTM raffinné avec Dropout (1 couche)\")\n",
    "plt.xlabel(\"Valeurs réelles\")\n",
    "plt.ylabel(\"Valeurs prédites\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(\"La RMSE du modèle est :\", rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que les premiers points prédits sont proches des valeurs réelles mais que le modèle ne parvient pas à prédire les hautes valeurs. Peut-on augmenter le dropout pour obtenir de meilleurs résultats ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [],
   "source": [
    "model = create_dropout_LSTM_model(dropout = 0.4)\n",
    "history = model.fit(X_train,\n",
    "                    y_train,\n",
    "                    validation_data = (X_test, y_test),\n",
    "                    batch_size = batch_size,\n",
    "                    epochs = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = np.linspace(1, 200, 200)\n",
    "losses = history.history[\"loss\"] # perte du jeu d'entraînement\n",
    "val_losses = history.history[\"val_loss\"] # perte du jeu de test\n",
    "plt.plot(epochs, losses, \"xb\")\n",
    "plt.plot(epochs, val_losses, \"xr\")\n",
    "plt.title(\"Perte en fonction des époques\")\n",
    "plt.xlabel(\"Epoques\")\n",
    "plt.ylabel(\"Perte (Mean Squared Error)\")\n",
    "plt.legend([\"Entraînement\", \"Test\"])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred[:, -1, 0]\n",
    "print(y_pred, y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(y_test, y_pred)\n",
    "plt.plot(np.linspace(np.min(y_test),np.max(y_test), len(y_pred)),\n",
    "        np.linspace(np.min(y_test),np.max(y_test), len(y_pred)),\n",
    "        color = \"r\")\n",
    "plt.title(\"Prédictions du réseau LSTM raffinné avec Dropout (1 couche)\")\n",
    "plt.xlabel(\"Valeurs réelles\")\n",
    "plt.ylabel(\"Valeurs prédites\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = mean_squared_error(y_test, y_pred, squared=False)\n",
    "print(\"La RMSE du modèle est :\", rmse)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "case_study_rnn_group_n",
   "notebookOrigID": 4256609446685350,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3.7.4  ('mds_env': venv)",
   "name": "pythonjvsc74a57bd0e4c4d4c6919227a365403d194a6bdeb75148d16854d2a03fdb72f37e9580a67c"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "metadata": {
   "interpreter": {
    "hash": "e4c4d4c6919227a365403d194a6bdeb75148d16854d2a03fdb72f37e9580a67c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
